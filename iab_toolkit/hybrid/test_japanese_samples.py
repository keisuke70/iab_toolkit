#!/usr/bin/env python3
"""
Comprehensive test using all Japanese text sample files to validate
the hybrid classification system with .npy embeddings.
"""

import time
from pathlib import Path
from .hybrid_iab_classifier import HybridIABClassifier

def load_japanese_samples():
    """Load all Japanese sample files."""
    base_path = Path(__file__).parent / "data"
    
    samples = {}
    sample_files = {
        "japanese_beauty_sample.txt": {
            "name": "Beauty & Cosmetics",
            "expected_tier1": "Style & Fashion",
            "description": "ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯åŒ–ç²§å“ãƒ–ãƒ©ãƒ³ãƒ‰"
        },
        "japanese_text_sample.txt": {
            "name": "Automotive",
            "expected_tier1": "Automotive", 
            "description": "ãƒˆãƒ¨ã‚¿RAV4 SUV"
        },
        "japanese_technology_sample.txt": {
            "name": "Technology",
            "expected_tier1": "Technology & Computing",
            "description": "iPhone 15 ProæŠ€è¡“"
        },
        "japanese_business_sample.txt": {
            "name": "Business & Finance",
            "expected_tier1": "Business and Finance",
            "description": "æ±äº¬è¨¼åˆ¸å–å¼•æ‰€ä¼æ¥­"
        },
        "japanese_health_sample.txt": {
            "name": "Health & Wellness",
            "expected_tier1": "Healthy Living",
            "description": "æ±äº¬å¤§å­¦åŒ»å­¦éƒ¨ç ”ç©¶"
        }
    }
    
    for filename, info in sample_files.items():
        file_path = base_path / filename
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read().strip()
                samples[filename] = {
                    "text": text,
                    "name": info["name"],
                    "expected_tier1": info["expected_tier1"],
                    "description": info["description"]
                }
        else:
            print(f"âš ï¸  Warning: Sample file not found: {filename}")
    
    return samples

def test_japanese_classification():
    """Test the hybrid classifier with all Japanese samples."""
    
    print("=" * 80)
    print("COMPREHENSIVE JAPANESE TEXT CLASSIFICATION TEST")
    print("=" * 80)
    print("ğŸ¯ Testing hybrid system with real Japanese content")
    print("ğŸ“Š Using .npy embeddings for tier1 + LLM for tier2")
    print("ğŸ‡¯ğŸ‡µ All samples are authentic Japanese text")
    print()
    
    # Initialize classifier
    print("Initializing HybridIABClassifier...")
    classifier = HybridIABClassifier()
    print()
    
    # Load Japanese samples
    samples = load_japanese_samples()
    
    if not samples:
        print("âŒ No Japanese sample files found!")
        return
    
    print(f"ğŸ“„ Loaded {len(samples)} Japanese sample files")
    print()
    
    # Test each sample
    results = []
    total_time = 0
    
    for i, (filename, sample) in enumerate(samples.items(), 1):
        text = sample["text"]
        name = sample["name"]
        expected = sample["expected_tier1"]
        description = sample["description"]
        
        print(f"ğŸ§ª Test {i}: {name}")
        print("=" * 60)
        print(f"ğŸ“ èª¬æ˜: {description}")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        print(f"ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹åˆ†é¡: {expected}")
        print(f"ğŸ“– ãƒ†ã‚­ã‚¹ãƒˆæ¦‚è¦: {text[:100]}...")
        print()
        
        # Perform classification
        start_time = time.time()
        result = classifier.classify(text)
        classification_time = time.time() - start_time
        total_time += classification_time
        
        # Check tier1 accuracy
        tier1_correct = result.primary_tier1_domain == expected
        status_emoji = "âœ…" if tier1_correct else "âš ï¸"
        
        print(f"ğŸ¯ åˆ†é¡çµæœ:")
        print(f"   {status_emoji} Tier1ãƒ‰ãƒ¡ã‚¤ãƒ³: {result.primary_tier1_domain}")
        print(f"   ğŸ“ˆ æœŸå¾…å€¤: {expected}")
        print(f"   âœ“ æ­£ç¢ºæ€§: {'æ­£è§£' if tier1_correct else 'ç•°ãªã‚‹çµæœ'}")
        print()
        
        # Show tier2 categories
        if result.tier2_categories:
            print(f"ğŸ·ï¸  Tier2ã‚«ãƒ†ã‚´ãƒª (TOP {len(result.tier2_categories)}):")
            for j, cat in enumerate(result.tier2_categories, 1):
                name_cat = cat.get('name', 'Unknown')
                confidence = cat.get('confidence', 0.0)
                print(f"   {j}. {name_cat} ({confidence:.1%})")
        else:
            print("ğŸ·ï¸  Tier2ã‚«ãƒ†ã‚´ãƒª: ãªã—")
        print()
        
        # Show user profile
        profile = result.user_profile
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"   å¹´é½¢å±¤: {profile.age_range}")
        print(f"   æŠ€è¡“ãƒ¬ãƒ™ãƒ«: {profile.geekiness_level}/10")
        print(f"   ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ´—ç·´åº¦: {profile.content_sophistication}")
        print(f"   é–¢å¿ƒåˆ†é‡: {', '.join(profile.interests[:3])}" + ("..." if len(profile.interests) > 3 else ""))
        print()
        
        # Performance
        print(f"â±ï¸  å‡¦ç†æ™‚é–“: {classification_time:.3f}ç§’")
        print()
        
        # Store results
        results.append({
            'filename': filename,
            'name': name,
            'description': description,
            'expected': expected,
            'detected': result.primary_tier1_domain,
            'tier1_correct': tier1_correct,
            'tier2_count': len(result.tier2_categories),
            'processing_time': classification_time,
            'user_profile': {
                'age_range': profile.age_range,
                'geekiness': profile.geekiness_level,
                'sophistication': profile.content_sophistication
            }
        })
        
        print("-" * 60)
        print()
    
    # Summary
    print("=" * 80)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    
    correct_count = sum(1 for r in results if r['tier1_correct'])
    accuracy = (correct_count / len(results)) * 100 if results else 0
    avg_time = total_time / len(results) if results else 0
    avg_geekiness = sum(r['user_profile']['geekiness'] for r in results) / len(results) if results else 0
    
    print(f"ğŸ“ˆ ãƒ†ã‚¹ãƒˆå®Œäº†: {len(results)}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«")
    print(f"âœ… Tier1æ­£ç¢ºæ€§: {correct_count}/{len(results)} ({accuracy:.1f}%)")
    print(f"â±ï¸  å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.3f}ç§’")
    print(f"ğŸ¯ å¹³å‡æŠ€è¡“ãƒ¬ãƒ™ãƒ«: {avg_geekiness:.1f}/10")
    print(f"ğŸš€ ç·å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
    print()
    
    # Detailed results
    print("ğŸ“‹ è©³ç´°çµæœ:")
    print("-" * 80)
    for result in results:
        status = "âœ…" if result['tier1_correct'] else "âš ï¸"
        print(f"{status} {result['name']}")
        print(f"   æœŸå¾…å€¤: {result['expected']}")
        print(f"   æ¤œå‡ºå€¤: {result['detected']}")
        print(f"   Tier2æ•°: {result['tier2_count']}")
        print(f"   å‡¦ç†æ™‚é–“: {result['processing_time']:.3f}ç§’")
        print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼: {result['user_profile']['age_range']}, "
              f"æŠ€è¡“Lv{result['user_profile']['geekiness']}, "
              f"{result['user_profile']['sophistication']}")
        print()
    
    # System status
    print("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:")
    print("=" * 80)
    print("âœ… .npyåŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨ä¸­")
    print("âœ… æ—¥æœ¬èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œæ¸ˆã¿")
    print("âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†é¡ç¨¼åƒä¸­")
    print("âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æ©Ÿèƒ½")
    print("âœ… æœ¬ç•ªç’°å¢ƒå¯¾å¿œæº–å‚™å®Œäº†")
    
    if accuracy == 100:
        print()
        print("ğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        print("ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ ã¯å®Œå…¨ã«æ©Ÿèƒ½ã—ã¦ã„ã¾ã™")
    elif accuracy >= 80:
        print()
        print("ğŸ‘ ãƒ†ã‚¹ãƒˆã¯æ¦‚ã­æˆåŠŸã§ã™")
        print("ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    else:
        print()
        print("âš ï¸  ã„ãã¤ã‹ã®æ”¹å–„ãŒå¿…è¦ã§ã™")
    
    return results

def main():
    """Main entry point for the CLI command."""
    test_japanese_classification()

if __name__ == "__main__":
    main()
